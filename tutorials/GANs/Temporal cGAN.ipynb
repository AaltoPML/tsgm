{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882424d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "latent_dim = 1\n",
    "feature_dim = 1\n",
    "seq_len = 123\n",
    "output_dim = 1\n",
    "\n",
    "generator_in_channels = latent_dim + output_dim\n",
    "discriminator_in_channels = feature_dim + output_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential([\n",
    "        keras.layers.InputLayer((seq_len, discriminator_in_channels)),\n",
    "        layers.Conv1D(64, 3, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv1D(128, 3, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv1D(128, 3, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv1D(128, 3, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalAvgPool1D(),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential([\n",
    "        keras.layers.InputLayer((seq_len, generator_in_channels)),\n",
    "\n",
    "        layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(rate=0.2),\n",
    "    \n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(rate=0.2),\n",
    "    \n",
    "        layers.AveragePooling1D(pool_size=57, strides=57),\n",
    "        layers.LocallyConnected1D(1, 1, activation=\"tanh\"),\n",
    "    \n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e04cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_input = keras.Input((seq_len, generator_in_channels))\n",
    "x = layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\")(g_input)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)        \n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "pool_and_stride = round((x.shape[1] + 1) / (seq_len + 1))\n",
    "x = layers.AveragePooling1D(pool_size=pool_and_stride, strides=pool_and_stride)(x)\n",
    "g_output = layers.LocallyConnected1D(1, 1, activation=\"tanh\")(x)\n",
    "\n",
    "generator = keras.Model(g_input, g_output, name=\"generator\")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_input = keras.Input((seq_len, discriminator_in_channels))\n",
    "x = layers.Conv1D(64, 3, strides=2, padding=\"same\")(d_input)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.Conv1D(128, 3, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.Conv1D(128, 3, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.Conv1D(128, 3, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.GlobalAvgPool1D()(x)\n",
    "d_output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "discriminator = keras.Model(d_input, d_output, name=\"discriminator\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None,]\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, seq_len, output_dim)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.        \n",
    "        tmp_latent = tf.random.normal(shape=(batch_size, seq_len, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [tmp_latent, one_hot_labels[:, :, None]], axis=2\n",
    "        )\n",
    "        \n",
    "        # TODO: experiment\n",
    "        #random_vector_labels = one_hot_labels[:, :, None]\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        #tmp_latent = tf.repeat(\n",
    "        #    random_latent_vectors[:, None, :], repeats=[seq_len], axis=1\n",
    "        #)\n",
    "        \n",
    "        tmp_latent = tf.random.normal(shape=(batch_size, seq_len, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [tmp_latent, one_hot_labels[:, :, None]], axis=2\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa869807",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tsgm.utils.gen_sine_const_switch_dataset(50_000, seq_len, 1, max_value=20, const=10)\n",
    "\n",
    "scaler = tsgm.utils.TSFeatureWiseScaler((-1, 1))\n",
    "X_train = scaler.fit_transform(X)\n",
    "\n",
    "#scaler_y = tss.utils.TSFeatureWiseScaler((-1, 1))\n",
    "#y = scaler_y.fit_transform(y)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9aad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_samples: int, latent_dim: int = 128, num_classes: int = 2,\n",
    "                 save: bool = True, save_path: typing.Optional[str] = None):\n",
    "        self._num_samples = num_samples\n",
    "        self._latent_dim = latent_dim\n",
    "        self._num_classes = num_classes\n",
    "        self._save = save\n",
    "        self._save_path = save_path\n",
    "\n",
    "        if self._save and self._save_path is None:\n",
    "            self._save_path = \"/tmp/\"\n",
    "            print(\"[WARNING]: save_path is not specified. Using `/tmp` as the default save_path\")\n",
    "\n",
    "        if self._save is False and self._save_path is not None:\n",
    "            print(\"[WARNING]: save_path is specified, but save is False.\")\n",
    "            os.makedirs(self._save_path, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self._num_classes * self._num_samples, self._latent_dim))\n",
    "\n",
    "        labels = []\n",
    "        for i in range(self._num_classes):\n",
    "            if not len(labels):\n",
    "                labels = keras.utils.to_categorical([i], self._num_classes)\n",
    "            else:\n",
    "                labels = tf.concat((labels, keras.utils.to_categorical([i], self._num_classes)), 0)\n",
    "\n",
    "        labels = tf.repeat(labels, self._num_samples, axis=0)\n",
    "        generated_images = self.model.generator(tf.concat([random_latent_vectors, labels], 1))\n",
    "\n",
    "        for i in range(self._num_classes * self._num_samples):\n",
    "            sns.lineplot(x=range(0, generated_images[i].shape[0]), y=tf.squeeze(generated_images[i]))\n",
    "            if self._save:\n",
    "                plt.savefig(\"/tmp/epoch_{}_sample_{}\".format(epoch, i))\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "cbk = GANMonitor(num_samples=2, latent_dim=latent_dim)\n",
    "cond_gan.fit(dataset, epochs=10000)#, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f694b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsgm.utils.visualize_ts_lineplot(X_train, y, 5)\n",
    "plt.savefig(\"data_temporal_gan.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd598d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "\n",
    "tmp_latent = tf.random.normal(shape=(n_samples, seq_len, latent_dim))\n",
    "random_vector_labels = tf.concat(\n",
    "    [tmp_latent, y[:n_samples, :, None]], axis=2\n",
    ")\n",
    "\n",
    "generated_images = cond_gan.generator(random_vector_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsgm.utils.visualize_ts_lineplot(generated_images, y, 5)\n",
    "plt.savefig(\"synth_data_temporal_gan.pdf\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
