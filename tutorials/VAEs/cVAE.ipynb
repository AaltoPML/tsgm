{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dde101",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c512cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tsgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "feat_dim = 1\n",
    "num_classes = 2\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cVAE_CONV5Architecture(object):\n",
    "    def __init__(self, seq_len, feat_dim, latent_dim, output_dims=2):\n",
    "        self._seq_len = seq_len\n",
    "        self._feat_dim = feat_dim\n",
    "        self._latent_dim = latent_dim\n",
    "        self._output_dims = output_dims\n",
    "        \n",
    "        self._encoder = self._build_encoder()\n",
    "        self._decoder = self._build_decoder()\n",
    "        \n",
    "    def _build_encoder(self):\n",
    "        encoder_inputs = keras.Input(shape=(self._seq_len, self._feat_dim + self._output_dims))\n",
    "\n",
    "        x = layers.Conv1D(64, 10, activation=\"relu\", strides=1, padding=\"same\")(encoder_inputs)\n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "        x = layers.Conv1D(64, 2, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "        x = layers.Conv1D(64, 2, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "        x = layers.Conv1D(64, 2, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "        x = layers.Conv1D(64, 4, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        z_mean = layers.Dense(self._seq_len * self._latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(self._seq_len * self._latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "        return encoder\n",
    "        \n",
    "    def _build_decoder(self):\n",
    "        inputs = keras.Input(shape=(self._seq_len, self._latent_dim + self._output_dims,))\n",
    "        x = layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\")(inputs)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)        \n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "        x = layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\")(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "        x = layers.Conv1DTranspose(64, 2, strides=2, padding=\"same\")(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "        pool_and_stride = round((x.shape[1] + 1) / (seq_len + 1))\n",
    "        x = layers.AveragePooling1D(pool_size=pool_and_stride, strides=pool_and_stride)(x)\n",
    "        d_output = layers.LocallyConnected1D(1, 1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        decoder = keras.Model(inputs, d_output, name=\"decoder\")\n",
    "        return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cBetaVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, latent_dim, temporal, beta=1.0, **kwargs):\n",
    "        super(cBetaVAE, self).__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")     \n",
    "        self.latent_dim = latent_dim\n",
    "        self._temporal = temporal\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def generate(self, labels):\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        z = tf.random.normal((batch_size, seq_len, self.latent_dim))\n",
    "        decoder_input = self._get_decoder_input(z, labels)\n",
    "        return (self.decoder(decoder_input), labels)\n",
    "    \n",
    "    def call(self, data):\n",
    "        X, labels = data\n",
    "        encoder_input = self._get_encoder_input(X, labels)\n",
    "        z_mean, _, _ = self.encoder(encoder_input)\n",
    "        \n",
    "        decoder_input = self._get_decoder_input(z_mean, labels)\n",
    "        x_decoded = self.decoder(decoder_input)\n",
    "        if len(x_decoded.shape) == 1:\n",
    "            x_decoded = x_decoded.reshape((1, -1))\n",
    "        return x_decoded\n",
    "    \n",
    "    def _get_reconstruction_loss(self, X, Xr):\n",
    "        reconst_loss = tf.reduce_sum(tf.math.squared_difference(X, Xr)) +\\\n",
    "                       tf.reduce_sum(tf.math.squared_difference(tf.reduce_mean(X, axis=1), tf.reduce_mean(Xr, axis=1))) +\\\n",
    "                       tf.reduce_sum(tf.math.squared_difference(tf.reduce_mean(X, axis=2), tf.reduce_mean(Xr, axis=2)))\n",
    "        return reconst_loss\n",
    "    \n",
    "    def _get_encoder_input(self, X, labels):\n",
    "        if self._temporal:\n",
    "            return tf.concat([X, labels[:, :, None]], axis=2)\n",
    "        else:\n",
    "            rep_labels = tf.repeat(labels[:, None, :], [seq_len], axis=1)\n",
    "            return tf.concat([X, rep_labels], axis=2)\n",
    "    \n",
    "    def _get_decoder_input(self, z, labels):\n",
    "        if self._temporal:\n",
    "            rep_labels = labels[:, :, None]\n",
    "        else:\n",
    "            rep_labels = tf.repeat(labels[:, None, :], [seq_len], axis=1)\n",
    "        z = tf.reshape(z, [-1, seq_len, latent_dim])\n",
    "        return tf.concat([z, rep_labels], axis=2)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        X, labels = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoder_input = self._get_encoder_input(X, labels)\n",
    "            z_mean, z_log_var, z = self.encoder(encoder_input)\n",
    "            \n",
    "            decoder_input = self._get_decoder_input(z_mean, labels)\n",
    "            reconstruction = self.decoder(decoder_input)\n",
    "            reconstruction_loss = self._get_reconstruction_loss(X, reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = cVAE_CONV5Architecture(seq_len=seq_len, feat_dim=feat_dim, latent_dim=latent_dim)\n",
    "\n",
    "encoder, decoder = architecture._encoder, architecture._decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27836fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff205e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_i = tsgm.utils.gen_sine_vs_const_dataset(5000, 256, 1, max_value=20, const=10)\n",
    "\n",
    "scaler = tsgm.utils.TSFeatureWiseScaler((0, 1))\n",
    "X = scaler.fit_transform(X).astype(np.float32)\n",
    "y = keras.utils.to_categorical(y_i, num_classes).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41332564",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = architecture._encoder, architecture._decoder\n",
    "\n",
    "vae = cBetaVAE(encoder, decoder, latent_dim=latent_dim, temporal=False)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0003,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.9)\n",
    "optimizer = keras.optimizers.Adam(lr_schedule)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "vae.fit(X, y, epochs=1000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a658082",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_decoded = vae.predict([X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e18cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gen, y_gen = vae.generate(y[:limit])\n",
    "X_gen = X_gen.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a577ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_original_and_reconst_ts(original, reconst, num=5, vmin=0, vmax=1):\n",
    "    assert original.shape == reconst.shape\n",
    "\n",
    "    fig, axs = plt.subplots(num, 2, figsize=(14, 10))\n",
    "\n",
    "    ids = np.random.choice(original.shape[0], size=num, replace=False)\n",
    "    for i, sample_id in enumerate(ids):\n",
    "        axs[i, 0].imshow(original[sample_id].T, aspect='auto', vmin=vmin, vmax=vmax)\n",
    "        axs[i, 1].imshow(reconst[sample_id].T, aspect='auto', vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_original_and_reconst_ts(X, x_decoded, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26758199",
   "metadata": {},
   "source": [
    "## Visualize using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "limit = 1000\n",
    "X_sub, y_sub = X[:limit], y[:limit]\n",
    "tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random')\n",
    "\n",
    "X_all = np.concatenate((X_sub, X_gen))\n",
    "y_all = np.concatenate((y_sub, y_gen))\n",
    "c = np.argmax(np.concatenate((y_sub, y_gen)), axis=1)\n",
    "colors = {0: \"class 0\", 1: \"class 1\"}\n",
    "c = [colors[el] for el in c]\n",
    "point_styles = [\"hist\"] * X_sub.shape[0] + [\"gen\"] * X_gen.shape[0]\n",
    "X_emb = tsne.fit_transform(np.resize(X_all, (X_all.shape[0], X_all.shape[1] * X_all.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62deff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "sns.scatterplot(x=X_emb[:, 0], y=X_emb[:, 1], hue=c[:], style=point_styles[:], markers={\"hist\": \"<\", \"gen\": \"H\"}, alpha=0.7)\n",
    "#sns.scatterplot(x=X_emb[limit:, 0], y=X_emb[limit:, 1], hue=c[limit:], style=point_styles[limit:], marker=\"s\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.box(False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
